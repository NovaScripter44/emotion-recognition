# ğŸ­ Emotion Recognition from Speech (RAVDESS Dataset)

This project is a complete end-to-end pipeline for **speech/song emotion recognition**, built using:

- Deep Learning (CNNs)
- Audio feature extraction using **MFCCs + Delta**
- Interactive **Streamlit app** to test by uploading audio
- Trained and tested using the **RAVDESS dataset**

---

```
emotion-recognition/
â”œâ”€â”€ app.py # ğŸŒ Streamlit app
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ emotion_cnn_model.h5 # ğŸ¤– Trained CNN model
â”‚ â””â”€â”€ label_encoder.pkl # ğŸ·ï¸ LabelEncoder for emotion classes
â”œâ”€â”€ utils.py # âš™ï¸ Feature extraction (MFCC, delta), helpers
â”œâ”€â”€ requirements.txt # ğŸ“¦ Python dependencies
â”œâ”€â”€ test_audio/
â”‚ â””â”€â”€ test.wav # ğŸ™ï¸ Sample test audio
â”œâ”€â”€ temp_wav/ # ğŸ”Š Stores temp audio files
â”œâ”€â”€ emotion-recognition_notebook.ipynb # ğŸ““ Training + experiments notebook
â””â”€â”€ test_model.py # ğŸ§ª CLI script for testing model on sample
```



---

## ğŸš€ Features

- ğŸ§ Accepts uploaded `.wav` files
- ğŸ¯ Trained on 8 emotion classes:
  `['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']`
- ğŸ“Š Achieved >80% accuracy and macro F1 score and >=75% accuracy in other classes
- ğŸ“ˆ Early stopping, class balancing, and MFCC + Delta stacking used for best performance

---

## ğŸ› ï¸ How It Works

### ğŸ”‰ Feature Extraction

- We use **Librosa** to extract:
  - `MFCCs`: Mel-Frequency Cepstral Coefficients
  - `Delta`: First-order temporal derivative of MFCCs
- For CNN:
  - MFCC + Delta stacked vertically
  - Input shape: `(80, 300, 1)` â€” 80 features over 300 time steps

### ğŸ§  Model (CNN)

```python
Sequential([
    Conv2D(32), MaxPooling2D, BatchNorm,
    Conv2D(64), MaxPooling2D, BatchNorm,
    Conv2D(128), MaxPooling2D, BatchNorm,
    GlobalAveragePooling2D,
    Dense(128, relu) + Dropout(0.3),
    Dense(num_classes, softmax)
])
Optimizer: Adam

Loss: categorical_crossentropy

EarlyStopping and class_weight balancing added
```
---

## Streamlit App Usage:
### Activate virtual environment
```
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/macOS

### Install dependencies
pip install -r requirements.txt

### Run app
streamlit run app.py
```
---

## Testing using test_model.py :
python test_model.py

## ğŸ“Œ Dataset Used:
```
https://zenodo.org/records/1188976#.XCx-tc9KhQI
-Only audio files (not video) were used.
-Contains recordings by 24 actors in 8 emotion categories.

